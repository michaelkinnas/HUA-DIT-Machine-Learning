{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BeamSearchScorer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'we passed data mining'\n",
    "\n",
    "encoded_text = tokenizer(text, return_tensors='pt')\n",
    "encoded_text = encoded_text.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoders and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we passed data mining and data mining in general, and we have to be very careful about that. It's not just about the data, it's about how we use that data, how we do it.\n",
      "\n",
      "We're talking about a very important issue here: how we use data. We have a huge amount of data that we're not able to use in a very efficient way, because there's a lot of data that we can use in the same way, but it's very hard to use in a very efficient way. We've been using data for years, and we're very lucky to have data for a very large number of different things.\n",
      "\n",
      "So, we have to be really careful about what we're doing in the data. We've got to have a very clear vision about what we're doing. It's a lot harder when you're using a very small number of things.\n",
      "\n",
      "So, we've been working on this for a very long time. We're very excited by\n"
     ]
    }
   ],
   "source": [
    "#Default top_k\n",
    "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_k=4)\n",
    "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we passed data mining, and the project became known as DataRipper. We quickly saw that people just got paid $1000 per month to collect data on your emails at the end of their tenure. With the end of data mining, data mining was not a new business. After a while, people realized that it was just so far into the future that more and more data was being collected.\n",
      "\n",
      "The data harvesting trend that has hit the data-mining business is accelerating. In a few years, you may even see a big boom in the data-mining business.\n",
      "\n",
      "Let's compare the data mining boom to the data-marketing boom.\n",
      "\n",
      "Data mining is a new business model in the space. It has been going on for decades. You use data to collect data, collect data for use in other applications. People like us are paying a low price for the data they already collect. No one seems to realize how small the market is. Let's look at an example from an example where\n"
     ]
    }
   ],
   "source": [
    "#Default top_p\n",
    "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, top_p=90)\n",
    "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we passed data mining.\n",
      "\n",
      "The next step is to create a new user and add it to the account. We'll create the user in the same way as before, but we'll also add a password. This will allow us to login to our account and log in with the username and password we just created. After that, we need to set up a login process. Here's how we do it: First, let's create an account with a user name and an email address. In this case, I'm going to use my new password, which I'll use to sign in. Next, make sure that the password is correct. If it's not, you'll get an error message telling you that your password has been changed. You can try again later if you'd like. Now, create another user using your new username, and then login with that user's email. Finally, log into your account using the login screen. It should look something like this: Now that we have our user,\n"
     ]
    }
   ],
   "source": [
    "#Beam search\n",
    "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True, num_beams=4, early_stopping=True, no_repeat_ngram_size=2, )\n",
    "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we passed data mining that I knew could have made a major difference on how these data were distributed, and how people came up with these things. At one point, we found that just about any small set of variables that we could change could be used as a predictor of the distributions of individual variables,\" she explains.\n",
      "\n",
      "She says that she has now been able to develop and implement sophisticated tools for measuring the utility of data mining data. First, she has done a few tests using data to test the reliability of some of the assumptions we made when we started out. She also has conducted more studies to see how the two data sets might be used to construct a better predictor of the distribution of distributions of random variables like health.\n",
      "\n",
      "Next, she has been using her tools to identify and fix statistical errors that may in some other way make certain variables different from the one they represent â€” such as when a new variable appears in one dataset and then changes its representation as it moves into another. The tools she created to\n"
     ]
    }
   ],
   "source": [
    "#Greedy\n",
    "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True)\n",
    "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we passed data mining tests using Linux and Python\n",
      "\n",
      "Our goal was to work out the best way possible that we could for each of them to use the resources we collected as quickly as possible, using open source software (OOPs). That may sound harsh but these are not the kinds of tools to be used by our users.\n",
      "\n",
      "The primary goal is the removal of unnecessary dependencies and dependencies caused by using the Python toolchain without having to manually run any of their dependencies or manually compile a C library manually. All that is required is the following:\n",
      "\n",
      "1. Choose any OOPs you would like to use: oop or python\n",
      "\n",
      "or Python A.I.L.D. (all the examples are in the documentation; see the reference section for more details)\n",
      "\n",
      "or OOPs you would like to use: or Python\n",
      "\n",
      "and library(s); see the reference section for more details) When you run those dependencies or compiled one you will receive your error messages either\n"
     ]
    }
   ],
   "source": [
    "#random sampling\n",
    "response = model.generate(**encoded_text, max_new_tokens=200, do_sample=True)\n",
    "response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
